{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ensemble of Random Forests with Resampling\n",
    "\n",
    "In this notebook we use an enseble of random forest to predict the customer satisfaction. One of the more poignant characteristics of the data is the imbalance between the target classes, only ~4.5% of the observations are labeled as unsatisfied. To deal with this issue we undersample the majority class. The procedure is as follows (for details see train_forests.py):\n",
    "\n",
    "- Split the training data according to the target variable: happy and unhappy customers.\n",
    "\n",
    "- Randomly pick a specified fraction of the unhappy data.\n",
    "\n",
    "- Merge the previous observations with a equally sized random sample of happy observations.\n",
    "\n",
    "This gives us a balanced data set on which to train our random forests.\n",
    "\n",
    "Remark: The metric that we use for evaluation is the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We import custom utility functions for data processing and random forest training\n",
    "from process_data import process, create_submission, drop_columns\n",
    "from train_forests import trainForests, mean_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data/train_saldo.csv')\n",
    "data = pd.read_csv('data/train_extended_saldo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = train.ix[:,:-1], train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = test.ix[:,:-1], test['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Single Forest Classifier\n",
    "\n",
    "To have a better understanding of the problem, we examine the performance of a single random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 150)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf_bal = RandomForestClassifier(n_estimators = 150, class_weight='balanced')\n",
    "rf_bal.fit(X_train, Y_train)\n",
    "# custom undersambpling (see train_forests.py)\n",
    "rf_custom = trainForests(train, n_trees = 150)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rf\n",
    "Y_prob = pd.DataFrame(rf.predict_proba(X_test))[1]\n",
    "print(\"rf_score is {}\".format(roc_auc_score(Y_test, Y_prob)))\n",
    "# rf_bal\n",
    "Y_prob = pd.DataFrame(rf_bal.predict_proba(X_test))[1]\n",
    "print(\"rf_bal score is {}\".format(roc_auc_score(Y_test, Y_prob)))\n",
    "# rf_custom\n",
    "Y_prob = pd.DataFrame(rf_custom.predict_proba(X_test))[1]\n",
    "print(\"rf_custom score is {}\".format(roc_auc_score(Y_test, Y_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that the built-in parameter to deal with imbalanced data sets, 'class_weight', is not really useful. However, our custom resampling really makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.3 Feature Importance\n",
    "\n",
    "It is worthwile to try and understand what are the most important features according to a random forest classifier. One may even consider to train the final model using only some of the top features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_importance(n_trees, data):\n",
    "    rf = RandomForestClassifier(n_trees)\n",
    "    rf.fit(data.ix[:,:-1], data.ix[:,-1])\n",
    "    fimp = rf.feature_importances_\n",
    "    results = {}\n",
    "    for idx, name in enumerate(data.ix[:,:-1].columns):\n",
    "        results[name] = fimp[idx]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can use an initial random forest to take a look at important features\n",
    "if True: # Set to 'False' to skip this step as it takes some time.\n",
    "    threshold = 0.01\n",
    "    rank = feature_importance(300, data)\n",
    "    print(\"The rank of the features is as follows:\")\n",
    "    flag = 1\n",
    "    count = 0\n",
    "    for a in sorted(rank.keys(), key=rank.get)[::-1]:\n",
    "        aux = rank[a]\n",
    "        count+= 1\n",
    "        if aux < threshold and flag:\n",
    "            flag = 0\n",
    "            print('---'*5)\n",
    "            print('There are {} features above {} '.format(count, threshold))\n",
    "            print('---'*5)\n",
    "        print(a, '-->', rank[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Remark**: This backs up our data analysis. Notice how all the new features that we created are on the top of the importance rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Ensemble of Random Forests\n",
    "\n",
    "In the final part of the model, we train N_forest-many random forest with n_trees-many trees each. Ultimately, the prediction of the probability of being unsatisfied (TARGET = 0) is made as the geometric mean of the individual predictions.\n",
    "\n",
    "The basic parameters in the model are:\n",
    "- N_forest: number of random forests to train.\n",
    "- n_trees: number of trees for each forest.\n",
    "- a: the fraction of the TARGET == 1 class to be used in the training of each forest.\n",
    "- w: weight of the TARGET == 0 class in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We perform a grid-search to find the best parameters, we obtain a=0.25, w=1, N_forest=90, n_trees=200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_data = pd.read_csv(\"cross-validation/cross-val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_data[cv_data['auc_roc'] > 0.835]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compromise with N_forest = 30 and n_trees = 200 for the sake of\n",
    "computer performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0.25 # It can also be >1 to oversample the minority class (1 ie. unhappy)\n",
    "w = 1  # Weight of the majority class in the final sample.\n",
    "N_forest = 30  # Number of forests to train\n",
    "n_trees = 200  # Number of trees in each forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfs = trainForests(train, a, w, N_forest, n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean_ensemble takes the predicted probabilities of each random forest and computes the geometric mean. See train_forests.py for more details\n",
    "Y_tot = mean_ensemble(rfs, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = roc_auc_score(Y_test,Y_tot['geometric'])\n",
    "rf_classifiers = [rfs, temp]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(rf_classifiers, open(\"models/rf_classifier_param1.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we print the score of the prediction on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_prob = mean_ensemble(rfs, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(Y_train, Y_prob[\"geometric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that some overfitting might be going on. We are going to try and fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Help with overfitting: 'min_samples_leaf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another parameter that we can use to prevent overfitting, 'min_samples_leaf'. This is the number of samples that we require each leaf to have in each decision tree of the random forest. The higher the less prone to over fitting. We run a token experiment to see how scores change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for msl in range(1, 11):\n",
    "    rf = RandomForestClassifier(n_estimators = 150, min_samples_leaf = msl)\n",
    "    rf.fit(train.ix[:,:-1],train['TARGET'])\n",
    "    temp = pd.DataFrame(rf.predict_proba(X_test))[1]\n",
    "    print(\"msl={} --> {}\".format(msl,roc_auc_score(Y_test, temp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good choice for the parameter is msl = 3. Let's see whether it improves our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfs = trainForests(train, a, w, N_forest, n_trees, msl=3)\n",
    "print(rfs[-1].get_params)\n",
    "Y_tot = mean_ensemble(rfs, X_test)\n",
    "temp = roc_auc_score(Y_test,Y_tot['geometric'])\n",
    "rf_classifiers.append([rfs, temp])\n",
    "print(\"Test error = {}\".format(temp))\n",
    "Y_prob = mean_ensemble(rfs, X_train)\n",
    "print(\"Training error = {}\".format(roc_auc_score(Y_train, Y_prob[\"geometric\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the random forest classifiers trained above to create a meta predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = train.ix[:,:-1], train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for rf in rfs:\n",
    "    temp = rf.predict(train.ix[:,:-1])\n",
    "    temp = pd.DataFrame(temp)\n",
    "    X_train = pd.concat([X_train, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_meta = pd.concat([X_train, train['TARGET']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = test.ix[:,:-1], test['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rf in rfs:\n",
    "    temp = rf.predict(test.ix[:,:-1])\n",
    "    temp = pd.DataFrame(temp)\n",
    "    X_test = pd.concat([X_test, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = test['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random regularization\n",
    "cv = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    n_trees = np.random.randint(50,300)\n",
    "    lg = RandomForestClassifier(n_estimators = n_trees)\n",
    "    lg.fit(X_train, Y_train)\n",
    "    cv[n_trees] = roc_auc_score(Y_test, lg.predict(X_test))\n",
    "    print(cv[n_trees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in sorted(cv.keys(), key=cv.get)[::-1]:\n",
    "    print(key,cv[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = test.ix[:,:-1], test.ix[:,-1]\n",
    "n = 100\n",
    "a = 0.25\n",
    "w = 1\n",
    "N_forest = 5\n",
    "n_trees = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for _ in range(n):\n",
    "    rfs = trainForests(train, a, w, N_forest,n_trees)\n",
    "    Y_prob = mean_ensemble(rfs, X_test)\n",
    "    scores.append(roc_auc_score(Y_test,Y_prob))\n",
    "scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of scores\")\n",
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If desired, transform probabilities into class labels.\n",
    "def threshold(Y_prob, threshold = 0.5):\n",
    "    result = []\n",
    "    for y in Y_prob:\n",
    "        if y <= threshold:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate class labels\n",
    "Y_pred = threshold(Y_prob, threshold = 0.5)\n",
    "_ = eval_classification(test['TARGET'],Y_pred, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "def plot_features(forest):  \n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    n=len(indices)\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(n), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(n), indices)\n",
    "    plt.xlim([-1, n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrain forest on the whole 'train.csv' data\n",
    "rfs = trainForests(data, a, w, N_forest, n_trees, msl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "test_id = test.ix[:,'ID'].values\n",
    "process(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_prob = mean_ensemble(rfs,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(test_id, Y_prob['geometric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble RF and XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_boost = pd.read_csv('../Kaggle_Santander-master/simplexgbtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_boost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_rf = pd.read_csv('submissions/rforest_ensemble2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_prob = pd.concat([Y_boost,Y_rf.ix[:,'TARGET']], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_prob.rename(columns ={0:'ID', 1:'xgb', 2: 'rfe' }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# geometric mean ensemble\n",
    "l = 2 #number of predictors to ensemble\n",
    "temp = Y_prob.ix[:,1:].product(axis=1)\n",
    "temp = temp.apply(lambda x: np.power(x, 1./l))\n",
    "Y_prob['geometric'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arithmetic mean ensemble\n",
    "l = 2 #number of predictors to ensemble\n",
    "temp = Y_prob[['xgb', 'rfe']].mean(axis=1)\n",
    "temp = temp.apply(lambda x: np.power(x, 1./l))\n",
    "Y_prob['arithmetic'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# difference column\n",
    "temp = Y_prob['xgb'] - Y_prob['rfe']\n",
    "Y_prob['xgb - rfe'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference column\n",
    "temp = Y_prob['geometric'] - Y_prob['arithmetic']\n",
    "Y_prob['geo - ari'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Differences between XGB and RFE')\n",
    "plt.hist(Y_prob['xgb - rfe'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Differences between ensembles')\n",
    "plt.hist(Y_prob['geo - ari'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(test_id, Y_prob['arithmetic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
