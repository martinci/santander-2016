{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we take the approach of randomly undersampling the majority class (0 i.e. happy) to create balanced training data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from process_data import process\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_features(n_trees, data):\n",
    "    rf = RandomForestClassifier(n_trees)\n",
    "    rf.fit(data.ix[:,:-1], data.ix[:,-1])\n",
    "    fimp = rf.feature_importances_\n",
    "    important = []\n",
    "    for idx, name in enumerate(data.ix[:,:-1].columns):\n",
    "        if fimp[idx] > 0.1:\n",
    "            important.append(name)\n",
    "    return important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    selected = select_features(1000, data)\n",
    "    selected+= [\"TARGET\"]\n",
    "    data = data[selected]\n",
    "    len(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performace is similar wether we take 3 features or 300..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if submission:\n",
    "    train = data \n",
    "else:\n",
    "    train, test = train_test_split(data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = train[train.TARGET == 0]\n",
    "unhappy = train[train.TARGET == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_split(N,w):\n",
    "    if N<1:\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "    unhappy_temp = unhappy.sample(frac = 1/N, replace=flag)\n",
    "    size, _ = unhappy_temp.shape\n",
    "    if int(np.floor(w*size)) > happy.shape[0]:\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "    temp = happy.sample(int(np.floor(w*size)), replace = flag).append(unhappy_temp, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    return temp.ix[:,:-1], temp.ix[:,'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_ensemble(rfs,X_test):\n",
    "    df = pd.DataFrame()\n",
    "    for i, rf in enumerate(rfs):\n",
    "        temp = rfs[i].predict_proba(X_test)\n",
    "        Y_pred = pd.DataFrame(temp)[1]\n",
    "        df = pd.concat([df,Y_pred], axis = 1)\n",
    "    return df.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainForests(N, w, N_forest, n_trees, submission):\n",
    "    rfs = []\n",
    "    for i in range(N_forest):\n",
    "        temp =  RandomForestClassifier(n_trees, class_weight='balanced')\n",
    "        rfs.append(temp)\n",
    "    for i in range(N_forest):\n",
    "        X_train, Y_train = generate_split(N,w)\n",
    "        rfs[i].fit(X_train,Y_train)\n",
    "    if not submission:\n",
    "        Y_pred = mean_ensemble(rfs,test.ix[:,:-1])\n",
    "        return rfs, Y_pred\n",
    "    return rfs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_classification(Y_test, Y_pred, print_results = False):\n",
    "    # Y_pred needs to be  1 and 0's, not just probabilitys.\n",
    "    n = len(Y_test)\n",
    "    cm = confusion_matrix(Y_test,Y_pred)\n",
    "    tp = cm[1][1]  # True positives\n",
    "    fp = cm[0][1]  # False positives\n",
    "    fn = cm[1][0]  # False negatives\n",
    "    tn = cm[0][0]  # True negatives\n",
    "    print('TP={}, FP={}, FN={}, TN={}'.format(tp,fp,fn,tn))\n",
    "    miss = (fp + fn)/n    # missclassification error\n",
    "    accu = 1 -  miss      # accuracy\n",
    "    recall = tp/(tp + fn) # true positive rate (TPR), sensitivity, recall = True pos./(#real pos.)\n",
    "    spec = tn/(tn + fp)   # true negative rate (TNR), specificity = True neg./(#real neg.)\n",
    "    prec = tp/(tp + fp)   # precision = True pos./(#predicted pos.)\n",
    "    f1 = 2*(prec*recall)/(prec + recall) # F1 score\n",
    "    auc = roc_auc_score(Y_test, Y_pred)  # Area under the ROC curve.\n",
    "    \n",
    "    if print_results:\n",
    "        print(cm)\n",
    "        print(\"Missclasification error:\", miss)\n",
    "        print(\"Accuracy:\",accu)\n",
    "        print(\"Recall (Sensitivity, TPR):\", recall)\n",
    "        print(\"Specificity (TNR):\", spec)\n",
    "        print(\"Precision:\", prec)\n",
    "        print(\"F1-score:\", f1)\n",
    "        print(\"Area under ROC curve:\", auc)\n",
    "\n",
    "    return [miss, recall, spec, prec, f1, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold(Y_prob, threshold = 0.5):\n",
    "    result = []\n",
    "    for y in Y_prob:\n",
    "        if y <= threshold:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the cross-validation analysis, the best results are obtained with N=4, w=1, N_forest=60, n_trees=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 4 # N can also be <1 to also oversample the minority class (1 ie. unhappy)\n",
    "w = 1\n",
    "N_forest = 500\n",
    "n_trees = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfs, Y_prob = trainForests(N,w,N_forest,n_trees,submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83483121884515321"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test['TARGET'],Y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=454, FP=3164, FN=153, TN=11433\n",
      "[[11433  3164]\n",
      " [  153   454]]\n",
      "Missclasification error: 0.218166272034\n",
      "Accuracy: 0.781833727966\n",
      "Recall (Sensitivity, TPR): 0.747940691928\n",
      "Specificity (TNR): 0.78324313215\n",
      "Precision: 0.125483692648\n",
      "F1-score: 0.214911242604\n",
      "Area under ROC curve: 0.765591912039\n"
     ]
    }
   ],
   "source": [
    "# Project the probabilities into actual classifiers.\n",
    "Y_pred = threshold(Y_prob, threshold = 0.5)\n",
    "_ = eval_classification(test['TARGET'],Y_pred, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "def plot_features(forest, n):  \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(n), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(n), indices)\n",
    "    plt.xlim([-1, n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "test_id = test.ix[:,'ID'].values\n",
    "process(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = mean_ensemble(rfs,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\": test_id, \"TARGET\": Y_pred}).to_csv('submissions/rforest_ensemble_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
